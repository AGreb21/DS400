---
title: "Week 3, Lecture 1, Chapter 2"
format: html
editor: visual
---

```{r, message=FALSE}
library(bayesrules)
library(tidyverse)
library(janitor)
library(skimr)
```

#### Import data with bayesrules package

```{r}
data(fake_news)
```

#### Bring up dataset documentation

```{r}
?fake_news
```

#### Your Turn

ðŸ’» ðŸ“ˆ Take 10 minutes to do some some exploratory data analysis below and we will chat about best practices. Keep in mind two variables that we will focus on, type and title_has_excl

```{r}
# Quick structure and summary
str(fake_news)
summary(fake_news[, c("type", "title_has_excl")])

# Missing values
colSums(is.na(fake_news[, c("type", "title_has_excl")]))
```

```{r}
ggplot(fake_news, aes(x = type)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Distribution of Article Types",
    x = "Article Type",
    y = "Count"
  ) +
  theme_minimal()
```

```{r}
ggplot(fake_news, aes(x = (title_has_excl))) +
  geom_bar(fill = "darkorange") +
  labs(
    title = "Titles with Exclamation Marks",
    x = "Has Exclamation?",
    y = "Count"
  ) +
  theme_minimal()
```

```{r}
ggplot(fake_news, aes(x = type, fill = as.factor(title_has_excl))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Exclamation Marks by Article Type",
    x = "Article Type",
    y = "Percent within Type",
    fill = "Has Exclamation?"
  ) +
  theme_minimal()
```

```{r}
unique(fake_news$type)
unique(fake_news$title_has_excl)

```

```{r}

fake_news %>% 
  group_by(title_has_excl, type) %>% 
  count()

```

```{r}
ggplot(data = fake_news, aes(x = type, fill = title_has_excl)) +
  geom_bar()
```

### The Whole Game

![](images/clipboard-2218401115.png)

A \<- Has exclamation point

-   Ac \<- Does not have exclamation point

B \<- Article is fake

-   Bc \<- Article is real

![](images/clipboard-3013301639.png){width="534"}

Our fake news analysis boils down to the study of two **variables**: an articleâ€™s fake vs real status and its use of exclamation points. These features can *vary* from article to article. Some are fake, some arenâ€™t. Some use exclamation points, some donâ€™t. We can represent the *randomness* in these variables using **probability models**. In this section we will build a **prior probability model** for our prior understanding of whether the most recent article is fake; a model for interpreting the exclamation point **data**; and, eventually, a **posterior probability model** which summarizes the posterior plausibility that the article is fake.

### Prior Probability Model

Percent of articles fake vs real

P(B)

```{r}
fake_news %>% 
  tabyl(type) %>% 
  adorn_totals("row")

```

P(B) = 0.40

-   The prior probability (an [unconditional probability](https://www.bayesrulesbook.com/chapter-2#building-a-bayesian-model-for-events:~:text=two%20events.%20The-,unconditional%20probability,-of)) that the article is fake is 0.4

```{r}
prior_probability_article_fake <- 0.4
```

P(Bc) = 0.60

-   The prior probability (an unconditional probability) that the article is real is 0.6

P(A)

```{r}
fake_news %>% 
  tabyl(title_has_excl) %>% 
  adorn_totals("row")
```

P(A) = 0.12

-   The prior probability (an [unconditional probability](https://www.bayesrulesbook.com/chapter-2#building-a-bayesian-model-for-events:~:text=two%20events.%20The-,unconditional%20probability,-of)) that the article has an exclamation point is 0.12

```{r}
prior_probability_has_exc <- 0.12
```

P(Ac) = 0.88

-   The prior probability (an unconditional probability) that the article does not have an exclamation point is 0.88

So far we have this

$$
P(B \mid A) = \frac{0.4 L(B \mid A)}{0.12} = \frac{0.4 \cdot 0.2667}{0.12} = 0.889.
$$

#### Likelihood

-   The likelihood L(Bâˆ£A) is defined as P(Aâˆ£B):

    > *If the article were fake, how likely is it to have an exclamation point?*
    >
    > *The probability the article uses an exclamation point given that it is fake*
    >
    > -   What percentage of fake articles use an exclamation point?

```{r}
fake_news %>% 
  tabyl(title_has_excl, type) %>% 
  adorn_percentages("col") 
```

L(B\|A) = 0.267

```{r}
likelihood_article_fake_has_exc <- 0.27
```

L(Bc\|A) = 0.022

Now we have enough info for the posterior probability model

$$
P(B \mid A) = \frac{0.4 \cdot 0.2667  }{0.12} = \frac{0.4 \cdot 0.2667}{0.12} = 0.889.
$$

```{r}
(prior_probability_article_fake * likelihood_article_fake_has_exc) / prior_probability_has_exc
```

We started with a prior understanding that thereâ€™s only a 40% chance that the incoming article would be fake. Yet upon observing the use of an exclamation point in the title *â€œThe president has a funny secret!â€*, a feature thatâ€™s more common to fake news, our posterior understanding evolved quite a bit â€“ the chance that the article is fake jumped to 88.9%.

Another way to think about it:

P(B\|A)

```{r}
fake_news %>% 
  tabyl(title_has_excl, type) %>% 
  adorn_percentages("row") 
```

### Challenge

Come up with your own P(B\|A) and share with the class

Let's choose one, generate synthetic data, and model

```{r}
#Generarting random data set for being vacation and going to the beach
set.seed(123)  # reproducibility
n <- 1000      # sample size

# Define probabilities
p_vacation <- 0.3
p_beach_given_vacation <- 0.8
p_beach_given_not_vacation <- 0.2

# Generate vacation column (yes/no)
vacation <- sample(
  c("yes", "no"),
  size = n,
  replace = TRUE,
  prob = c(p_vacation, 1 - p_vacation)
)

# Generate beach_today column (yes/no) conditional on vacation
beach_today <- sapply(vacation, function(v) {
  if (v == "yes") {
    sample(c("yes", "no"), size = 1,
           prob = c(p_beach_given_vacation, 1 - p_beach_given_vacation))
  } else {
    sample(c("yes", "no"), size = 1,
           prob = c(p_beach_given_not_vacation, 1 - p_beach_given_not_vacation))
  }
})

# Combine into data frame
fake_beach_data <- data.frame(
  beach_today = factor(beach_today, levels = c("no", "yes")),
  vacation = factor(vacation, levels = c("no", "yes"))
)

# Preview first rows
head(fake_beach_data, 10)

# Summary counts
table(fake_beach_data$vacation, fake_beach_data$beach_today)
```

```{r}
view(fake_beach_data)

table(fake_beach_data$beach_today)
```

```{r}
ggplot(fake_beach_data, aes(x = beach_today, fill = vacation)) +
  geom_bar()
```

P(B) = This person went to the beach today

```{r}
fake_beach_data %>% 
  tabyl(beach_today) %>% 
  adorn_totals("row")
```

```{r}
probability_went_to_beach <- 0.36
```

P(A) = This person is on vacation

```{r}
fake_beach_data %>% 
  tabyl(vacation) %>% 
  adorn_totals("row")
```

```{r}
probabilty_on_vacation <- 0.30
```

L(B\|A) = P(A\|B) - The probability this person is on vacation given they went to the beach

```{r}
fake_beach_data %>% 
  tabyl(beach_today, vacation) %>% 
  adorn_percentages("row")
```

```{r}
likelihood_went_to_beach_on_vacation <- .63
```

P(B\|A) = What is the probability this person went to the beach given they are on vacation

```{r}
(probability_went_to_beach * likelihood_went_to_beach_on_vacation) / probabilty_on_vacation
probabilty_on_vacation
```

### Video

<https://www.youtube.com/watch?v=8vHKCrNGPhY>

### Challenge

Letâ€™s put Bayesâ€™ Rule into action in another example. Our word choices can reflect where we live. For example, suppose youâ€™re watching an interview of somebody that lives in the United States. Without knowing anything about this person, U.S. Census figures provide prior information about the region in which they might live: the Midwest (M), Northeast (N), South (S), or West (W).

```{r}
data(pop_vs_soda)
```

Exploratory Analysis

```{r}

```

```{r}

```

1\) Build a prior probability model (an unconditional probability model) with probabilities for where the person is from

-   Letting S denote the person is from the south

    -   P(S)

-   Letting N denote the person is from the northeast

    -   P(N)

-   Letting W denote the person is from the west

    -   P(W)

-   Letting M denote the person is from the midwest

    -   P(M)

2\) But then, you see the person point to a fizzy cola drink and say â€œplease pass my pop.â€ Though the country is united in its love of fizzy drinks, itâ€™s divided in what theyâ€™re called, with common regional terms including â€œpop,â€ â€œsoda,â€ and â€œcoke.â€ This **data**, i.e., the personâ€™s use of â€œpop,â€ provides further information about where they might live.

-   -Determine regional likelihoods that a person uses the word â€œpop,â€

    -   Letting A denote the event that a person uses the word â€œpop,â€

3\) Build a posterior probability model determining the probability that the person is from the south based on using the word "pop": P(S\|A)\

Pop vs Soda - In Class

```{r}
?pop_vs_soda
summary(pop_vs_soda)
# Count of responses by region
pop_vs_soda %>%
  count(region) %>%
  arrange(desc(n))

# Count of responses by word_for_cola
pop_vs_soda %>%
  count(word_for_cola) %>%
  arrange(desc(n))

# Cross-tabulation: region vs word_for_cola
pop_vs_soda %>%
  count(region, word_for_cola) %>%
  pivot_wider(names_from = word_for_cola, values_from = n, values_fill = 0)

# Proportion of "pop" vs "not pop"
pop_vs_soda %>%
  summarise(
    total = n(),
    pop_count = sum(pop),
    pop_prop = mean(pop)
  )
```

Exploratory Analysis

```{r}
# Visualization: Distribution of word_for_cola by region
ggplot(pop_vs_soda, aes(x = region, fill = word_for_cola)) +
  geom_bar() +
  labs(
    title = "Regional Preferences for Word for Cola",
    x = "Region",
    y = "Count"
  ) +
  theme_minimal()

# Visualization: Just "pop" usage by region
ggplot(pop_vs_soda, aes(x = region, fill = pop)) +
  geom_bar(position = "fill") +
  labs(
    title = "Proportion Saying 'Pop' by Region",
    x = "Region",
    y = "Proportion"
  ) +
  theme_minimal()
```

Prior Probability

P(B) Prior probability the person is from the south

```{r}
pop_vs_soda %>%
  tabyl(region)

```

```{r}
priors <- c(
  south = 0.39,
  midwest = 0.21,
  northeast = 0.17,
  west = 0.24
)

priors
```

```{r}
prior_probability_from_south <- 0.39
```

2\) Regional Likelihoods

```{r}
likelihoods <- pop_vs_soda %>%
  tabyl(pop, region) %>% 
  adorn_percentages("col") %>% 
  filter(pop == "TRUE") %>%
  select(-pop) %>% 
  unlist()
  
likelihoods
```

```{r}
probability_says_pop_given_from_south <- 0.079
```

3\) Marginal prior probability that a person uses the word pop

```{r}
prior_probabilty_says_pop <- sum(priors*likelihoods)
prior_probabilty_says_pop
```

4\) Finding our posterior probability P(S\|A)

```{r}
(prior_probability_from_south * probability_says_pop_given_from_south) / prior_probabilty_says_pop 
```

### Final challenge: map

```{r}
# Load libraries
library(dplyr)
library(ggplot2)
library(maps)       # US map data
library(mapdata)    # more detailed map outlines

# Step 1: Summarise % of "pop" users by state
state_pop <- pop_vs_soda %>%
  group_by(state) %>%
  summarise(
    total = n(),
    pop_count = sum(pop),
    pop_percent = 100 * mean(pop)
  )

# Step 2: Get US map data
us_states <- map_data("state")

# Step 3: Prepare state names for joining
# map_data("state") has lowercase state names
state_pop <- state_pop %>%
  mutate(state = tolower(state))

# Step 4: Join dataset with map
map_df <- us_states %>%
  left_join(state_pop, by = c("region" = "state"))

# Step 5: Plot map
ggplot(map_df, aes(long, lat, group = group, fill = pop_percent)) +
  geom_polygon(color = "white") +
  scale_fill_viridis_c(option = "plasma", na.value = "grey90") +
  labs(
    title = "Percentage of People Saying 'Pop' by State",
    fill = "% Saying 'Pop'"
  ) +
  coord_fixed(1.3) +
  theme_minimal()
```

```{r}

```
